# Architectures et ModÃ¨les de Calcul - Projet HPC

[![CI/CD Pipeline](https://github.com/salasss/Projet_Archi_Calcul/actions/workflows/ci.yml/badge.svg)](https://github.com/salasss/Projet_Archi_Calcul/actions)
[![Python 3.9+](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![C99+](https://img.shields.io/badge/C-99+-blue.svg)](https://en.cppreference.com/w/c)

## ğŸ“‹ Vue d'ensemble

Ce projet dÃ©montre deux concepts fondamentaux du calcul parallÃ¨le haute performance (HPC) :

1. **Part 1 : SIMD Vectorization (AVX-512)** - Multiplication matricielle optimisÃ©e
2. **Part 2 : Distributed Computing (MPI)** - Estimation du nombre Ï€ en parallÃ¨le

---

## Part 1 : Vectorisation SIMD (AVX-512)

### Objectif
ImplÃ©menter la multiplication matricielle C = A Ã— B en utilisant les intrinsÃ¨ques AVX-512.

### RÃ©sultats
```
Matrice: 512 Ã— 512
Version naÃ¯ve:    361.6 ms
Version AVX-512:   15.92 ms
Speedup:          22.71Ã—
```

### Fichiers
- `partie1_avx/matmul_naif.c` - Version naÃ¯ve (sans vectorisation)
- `partie1_avx/matmul_avx512.c` - Version optimisÃ©e AVX-512
- `partie1_avx/Makefile` - Script de compilation

### Techniques utilisÃ©es
- **Registres 512-bit** : Manipulation directe des __m512
- **FMA instruction** : `_mm512_fmadd_ps()` pour optimisation latence
- **Transposition matricielle** : AccÃ¨s mÃ©moire cache-friendly
- **16 floats parallÃ¨les** : Par instruction SIMD

### Compilation et test
```bash
cd partie1_avx
make
./matmul_avx
```

### Test avec Intel SDE (Ã©mulateur)
```bash
./sde64 -icl -- ./matmul_avx
```

---

## Part 2 : Calcul DistribuÃ© (MPI)

### Objectif
ImplÃ©menter un systÃ¨me client/serveur pour estimer Ï€ en utilisant la mÃ©thode Monte-Carlo.

### Architecture
```
Server (rank 0)
    â†“ (envoie "CONTINUE"/"STOP")
    â†‘ (reÃ§oit rÃ©sultats)
Client 1 (rank 1)  | Client 2 (rank 2)  | ... | Client N (rank N)
  (10M samples)      (10M samples)             (10M samples)
```

### RÃ©sultats
```
Clients | Ï€ estimate    | Error      | Samples   | Throughput
--------|---------------|------------|-----------|---------------
1       | 3.142355      | 0.000762   | 10M       | 5.1M/s
2       | 3.141021      | 0.000572   | 20M       | 9.2M/s
4       | 3.141539      | 0.000054   | 40M       | 21.2M/s
8       | 3.141704      | 0.000111   | 80M       | 27.6M/s
16      | 3.141572      | 0.000021   | 160M      | 29.4M/s
```

### Observations clÃ©s
- âœ“ **RÃ©duction d'erreur** : 97.2% (0.000762 â†’ 0.000021)
- âœ“ **ScalabilitÃ© linÃ©aire** : 16 clients = 16Ã— plus d'Ã©chantillons
- âœ“ **Convergence Monte-Carlo** : Suit la thÃ©orie 1/âˆšN
- âœ“ **Embarrassingly Parallel** : Overhead de communication minimal

### Fichiers
- `partie2_mpi/pi.py` - Programme principal client/serveur
- `partie2_mpi/benchmark.py` - Script d'automatisation des tests
- `partie2_mpi/report.py` - Analyse des rÃ©sultats
- `partie2_mpi/plot_results.py` - GÃ©nÃ©ration des graphiques
- `partie2_mpi/requirements.txt` - DÃ©pendances Python

### Installation et test
```bash
# Installation
pip install -r partie2_mpi/requirements.txt

# Test rapide (4 clients)
cd partie2_mpi
mpirun --oversubscribe -np 5 python3 pi.py

# Benchmark complet (1, 2, 4, 8, 16 clients)
python3 benchmark.py

# GÃ©nÃ©rer rapport et graphiques
python3 report.py
python3 plot_results.py
```

---

## ğŸ”„ CI/CD Pipeline

Ce projet inclut un pipeline GitHub Actions automatisÃ© qui :

âœ“ **Compile** les deux parties du projet (C + Python)  
âœ“ **Teste** les exÃ©cutables (Part 1 & Part 2)  
âœ“ **Valide** la qualitÃ© du code (flake8, black)  
âœ“ **VÃ©rifie** la documentation (README, requirements.txt)  
âœ“ **Archive** les artefacts de build  

### Statut du build
[![CI/CD Status](https://github.com/salasss/Projet_Archi_Calcul/actions/workflows/ci.yml/badge.svg)](https://github.com/salasss/Projet_Archi_Calcul/actions)

Le workflow s'exÃ©cute automatiquement Ã  chaque push sur `main` et `develop`.

---

## ğŸ“Š Fichiers gÃ©nÃ©rÃ©s

AprÃ¨s exÃ©cution de `python3 benchmark.py` et `python3 plot_results.py` :

- `benchmark_results.json` - DonnÃ©es brutes (5 configurations)
- `benchmark_results.png` - Graphiques de performance (4 sous-graphiques)

---

## ğŸ“‹ Structure du projet

```
Projet_Archi_Calcul/
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ ci.yml                          # Pipeline CI/CD
â”œâ”€â”€ partie1_avx/
â”‚   â”œâ”€â”€ matmul_naif.c                   # Version naÃ¯ve
â”‚   â”œâ”€â”€ matmul_avx512.c                 # Version AVX-512
â”‚   â””â”€â”€ Makefile                        # Script compilation
â”œâ”€â”€ partie2_mpi/
â”‚   â”œâ”€â”€ pi.py                           # Programme principal
â”‚   â”œâ”€â”€ benchmark.py                    # Automation tests
â”‚   â”œâ”€â”€ report.py                       # Analyse rÃ©sultats
â”‚   â”œâ”€â”€ plot_results.py                 # Graphiques
â”‚   â”œâ”€â”€ requirements.txt                # DÃ©pendances
â”‚   â”œâ”€â”€ benchmark_results.json          # RÃ©sultats (gÃ©nÃ©rÃ©)
â”‚   â””â”€â”€ benchmark_results.png           # Graphiques (gÃ©nÃ©rÃ©)
â””â”€â”€ README.md                           # Ce fichier
```

---

## ğŸ› ï¸ DÃ©pendances

### Part 1 (AVX-512)
- gcc ou clang (C99+)
- Option `-mavx512f` pour compilation
- Optional: Intel SDE pour testing sur CPU sans AVX-512

### Part 2 (MPI)
- Python 3.9+
- OpenMPI ou MPICH
- Packages: `mpi4py`, `matplotlib`, `numpy`

Installer les dÃ©pendances Python:
```bash
pip install -r partie2_mpi/requirements.txt
```

---

## âœ… Checklist de validation

### Part 1
- âœ“ Version naÃ¯ve en C
- âœ“ Version AVX-512 optimisÃ©e
- âœ“ Utilisation correcte des intrinsÃ¨ques `_mm512_*`
- âœ“ Manipulation directe des registres
- âœ“ Tests sur hardware natif
- âœ“ Tests avec Intel SDE emulator
- âœ“ Speedup significatif (22.71Ã— natif, 5.68Ã— Ã©mulÃ©)

### Part 2
- âœ“ Architecture client/serveur
- âœ“ 10 millions d'Ã©chantillons par batch
- âœ“ Protocole: (Nin, Ntotal) â†’ "CONTINUE"/"STOP"
- âœ“ Terminaison sur erreur < 0.001
- âœ“ Timeout management (10 secondes)
- âœ“ ImplÃ©mentation mpi4py (send/recv)
- âœ“ ScalabilitÃ© linÃ©aire vÃ©rifiÃ©e (1-16 clients)
- âœ“ ThÃ©orie convergence Monte-Carlo validÃ©e (1/âˆšN)
- âœ“ Benchmarking automatisÃ©
- âœ“ Analyse performance et graphiques

### Documentation
- âœ“ README complet
- âœ“ RÃ©sumÃ© du projet
- âœ“ Instructions d'exÃ©cution
- âœ“ RÃ©sultats dÃ©taillÃ©s
- âœ“ Pipeline CI/CD

---

## ğŸ“ Contact

Pour toute question ou remarque sur le projet, consultez les issues GitHub ou contactez directement.

---

**DerniÃ¨re mise Ã  jour** : 4 janvier 2026  
**Statut** : âœ… Complet et validÃ©
